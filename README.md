# ğŸ¨ Hotel Reservation â€” End-to-End MLOps with Databricks

[![Platform Badge](https://img.shields.io/badge/PLATFORM-DATABRICKS-FF3621?style=for-the-badge&logo=databricks&logoColor=white)](https://databricks.com)
[![Lang Badge](https://img.shields.io/badge/LANGUAGE-PYTHON_3.12-3670A0?style=for-the-badge&logo=python&logoColor=ffdd54)](#)
[![Infra Badge](https://img.shields.io/badge/ENV-DEVBOX_|_UV_|_TASKFILE-6d7cff?style=for-the-badge&logo=dev.to&logoColor=white)](#)

## ğŸ“š Table of Contents

* [ğŸ§  Project Overview](#-project-overview)
* [ğŸ§° Technology Stack](#-technology-stack)
* [âš™ï¸ Installation & Setup](#ï¸-installation--setup)
* [ğŸ§© Repository Structure](#-repository-structure)
* [ğŸš€ Key Features](#-key-features)
* [âš™ï¸ Databricks Asset Bundle Workflow](#ï¸-databricks-asset-bundle-workflow)
* [ğŸ§ª Development & Testing Workflow](#-development--testing-workflow)
* [ğŸ” Reproduce Results](#-reproduce-results)
* [ğŸ§± Prerequisites](#-prerequisites)
* [ğŸ§¾ Configuration Example](#-configuration-example)
* [ğŸ“Š End-to-End Workflow](#-end-to-end-workflow)
* [ğŸ“š Documentation](#-documentation)
* [ğŸ§‘â€ğŸ’» Contributing](#-contributing)
* [ğŸ“œ License & Credits](#-license--credits)


## ğŸ§  Project Overview

An **end-to-end MLOps project** developed as part of the [Databricks Free Edition Hackathon](https://www.databricks.com/blog/databricks-free-edition-hackathon-show-world-whats-possible-data-and-ai), running from November 5 - November 14, 2025.
It automates the complete lifecycle of a **hotel reservation classification model**, from **data ingestion & preprocessing** to **model training, registration, deployment, and serving** â€” fully orchestrated on **Databricks Free Edition**.

This repository demonstrates:

* **Reproducible ML pipelines** using **Databricks, MLflow**, and **LogisticRegression**
* **Automated Databricks job workflows** using **Databricks Asset Bundles**
* **Multi-environment configuration** across **DEV / ACC / PRD**
* **Environment management & automation** with **Devbox**, **UV**, and **Taskfile**
* **CI/CD** using **GitHub Actions** or **GitLab CI** (builds, docs, tests)
* **Comprehensive testing** with **Pytest**, **Ruff**, and **pre-commit**
* **Documentation & Wiki integration** via **Sphinx**.

## ğŸ§° Technology Stack

### Core Components
![Python](https://img.shields.io/badge/python-3.12-3670A0?style=for-the-badge&logo=python&logoColor=ffdd54)
![Databricks](https://img.shields.io/badge/Databricks-FF3621?style=for-the-badge&logo=databricks&logoColor=white)
[![Streamlit Badge](https://img.shields.io/badge/STREAMLIT_APP-FF4B4B?style=for-the-badge&logo=streamlit&logoColor=white)](#)
[![PowerBI Badge](https://img.shields.io/badge/POWER_BI-D8B511?style=for-the-badge&logo=powerbi&logoColor=white)](https://powerbi.microsoft.com)

### Databricks Components
![MLflow](https://img.shields.io/badge/MLflow-0194E2?style=for-the-badge&logo=mlflow&logoColor=white)
[![Bundle Badge](https://img.shields.io/badge/DATABRICKS_ASSET_BUNDLE-FF3621?style=for-the-badge&logo=databricks&logoColor=white)](#)
[![Pipeline Badge](https://img.shields.io/badge/DATABRICKS_PIPELINE-FF3621?style=for-the-badge&logo=databricks&logoColor=white)](#)
[![Serving Badge](https://img.shields.io/badge/MODEL_SERVING_ENDPOINT-FF3621?style=for-the-badge&logo=databricks&logoColor=white)](#)
[![App Badge](https://img.shields.io/badge/DATABRICKS_APP-FF3621?style=for-the-badge&logo=databricks&logoColor=white)](#)


### Development Environment
![Devbox](https://img.shields.io/badge/Devbox-6d7cff?style=for-the-badge&logo=dev.to&logoColor=white)
![Taskfile](https://img.shields.io/badge/Taskfile-231F20?style=for-the-badge&logo=gnu-bash&logoColor=white)
![UV](https://img.shields.io/badge/UV_Package_Manager-181717?style=for-the-badge&logo=pypi&logoColor=white)

### Version Control & CI/CD
![GitHub](https://img.shields.io/badge/github-%23121011.svg?style=for-the-badge&logo=github&logoColor=white)
![GitLab](https://img.shields.io/badge/gitlab-%23181717.svg?style=for-the-badge&logo=gitlab&logoColor=white)
![Pre-commit](https://img.shields.io/badge/Pre--commit-FFBB00?style=for-the-badge&logo=git&logoColor=white)
![Commitizen](https://img.shields.io/badge/Commitizen-1E90FF?style=for-the-badge&logo=git&logoColor=white)

## âš™ï¸ Installation & Setup

### 1ï¸âƒ£ Clone the Repository
```bash
git clone <your-repo-url>
cd hotel_reservation
````

### 2ï¸âƒ£ Install Tooling

```bash
task install
```

### 3ï¸âƒ£ Open a Devbox Shell (if needed)

```bash
devbox shell
```

### 4ï¸âƒ£ Create and Sync Python Environment

```bash
task dev-install
```

### 5ï¸âƒ£ Configure Environment Variables

```bash
cp .env.template .env
# â†’ update with Databricks credentials, tokens, etc.
```

âš ï¸ **Security Note**: Never commit your `.env` file. Use [Databricks Secrets](https://docs.databricks.com/en/security/secrets/secrets.html) or a secure vault.


### 6ï¸âƒ£ Run Demo Pipeline

```bash
task demo
```

You can verify your setup with:

```bash
task lint
task test
```

## ğŸ§© Repository Structure

```
.
â”œâ”€â”€ CHANGELOG.md                   # Project changelog â€” version history and updates
â”œâ”€â”€ CONTRIBUTING                   # Contribution guidelines (commits, PRs, conventions)
â”œâ”€â”€ LICENCE                        # Project license and usage permissions
â”œâ”€â”€ README.md                      # Main documentation file
â”œâ”€â”€ Taskfile.yml                   # Task automation (install, test, lint, deploy)
â”œâ”€â”€ Taskfiles.md                   # Extended task documentation and pipeline overview
â”‚
â”œâ”€â”€ app/                           # Local inference or monitoring application (Streamlit / API)
â”‚   â”œâ”€â”€ app.py                     # Main Streamlit app for model serving
â”‚   â”œâ”€â”€ app.yml                    # Databricks Asset Bundle configuration for app deployment
â”‚   â”œâ”€â”€ app_monitoring.py          # Streamlit dashboard for model performance monitoring
â”‚   â”œâ”€â”€ hotel.jpg / hotel.png      # Static images for UI display
â”‚   â””â”€â”€ requirements.txt           # App-specific dependencies
â”‚
â”œâ”€â”€ data/                          # Data lifecycle folders (raw â†’ processed)
â”‚   â”œâ”€â”€ external/                  # External datasets (APIs, external sources)
â”‚   â”œâ”€â”€ interim/                   # Intermediate transformed data
â”‚   â”œâ”€â”€ processed/                 # Processed data ready for ML training
â”‚   â””â”€â”€ raw/                       # Original raw dataset(s)
â”‚
â”œâ”€â”€ databricks.yml                 # Main Databricks Asset Bundle definition (job orchestration)
â”œâ”€â”€ devbox.json                    # Devbox environment configuration (reproducible shell)
â”‚
â”œâ”€â”€ docs/                          # Sphinx documentation and technical references
â”‚   â”œâ”€â”€ README.md                  # Secondary documentation index
â”‚   â”œâ”€â”€ commands.rst               # CLI commands and Taskfile command reference
â”‚   â”œâ”€â”€ references/                # Technical references (Databricks, MLflow, etc.)
â”‚   â””â”€â”€ reports/                   # Reports and analytics
â”‚       â””â”€â”€ figures/               # Generated figures and plots
â”‚
â”œâ”€â”€ models/                        # Exported models or local MLflow registry cache
â”‚
â”œâ”€â”€ notebooks/                     # Databricks notebooks for prototyping and orchestration
â”‚   â”œâ”€â”€ process_data.py            # Data cleaning and processing
â”‚   â”œâ”€â”€ train_register_*.py        # Model training and registration (basic/custom/FE)
â”‚   â”œâ”€â”€ deploy_*.py                # Model deployment scripts (Databricks Serving)
â”‚   â”œâ”€â”€ predict_*.py               # Model inference and validation
â”‚   â”œâ”€â”€ create_monitoring.py       # Creates model monitoring pipelines
â”‚   â”œâ”€â”€ refresh_monitoring.py      # Refreshes monitoring metrics and dashboards
â”‚   â”œâ”€â”€ demo.py                    # End-to-end demonstration pipeline
â”‚   â””â”€â”€ utils/                     # Helper scripts for Databricks operations
â”‚       â”œâ”€â”€ run_upload_data.py     # Upload dataset to Databricks Volume
â”‚       â”œâ”€â”€ run_cleanup_mlflow_experiments.py # Clean old MLflow experiments
â”‚       â”œâ”€â”€ run_create_mlflow_workspace.py    # Initialize MLflow workspace and structure
â”‚       â””â”€â”€ run_cleanup_data.py    # Clean up Databricks data volumes
â”‚
â”œâ”€â”€ project_config.yml             # Multi-environment configuration (dev / acc / prd)
â”œâ”€â”€ pyproject.toml                 # Python project metadata and dependencies
â”‚
â”œâ”€â”€ resources/                     # Databricks YAML templates and job configurations
â”‚   â”œâ”€â”€ inference.app.yml          # Model inference service deployment
â”‚   â”œâ”€â”€ bundle_monitoring.yml      # Monitoring job bundle definition
â”‚   â”œâ”€â”€ initial_training_*.yml     # Initial model training jobs (basic/custom)
â”‚   â”œâ”€â”€ weekly_training_*.yml      # Weekly retraining workflows
â”‚
â”œâ”€â”€ scripts/                       # Executable Python scripts for CI/CD and bundles
â”‚   â”œâ”€â”€ 00.process_initial_data.py # Initial dataset preparation
â”‚   â”œâ”€â”€ 01.process_new_data.py     # Incremental data ingestion pipeline
â”‚   â”œâ”€â”€ 02.train_register_model.py # Train and register baseline model
â”‚   â”œâ”€â”€ 02.b.train_register_custom_model.py # Train and register tuned/custom model
â”‚   â”œâ”€â”€ 03.deploy_model_serving.py # Deploy model to Databricks Serving
â”‚   â”œâ”€â”€ 03.b.deploy_custom_model_serving.py # Deploy the custom model variant
â”‚   â”œâ”€â”€ 04.post_commit_status.py   # Post-build CI validation
â”‚   â””â”€â”€ 05.refresh_monitor.py      # Refresh MLflow model monitoring metrics
â”‚
â”œâ”€â”€ src/                           # Main Python package source code
â”‚   â””â”€â”€ hotel_reservation/
â”‚       â”œâ”€â”€ data/                  # Data ingestion, config loading, and upload helpers
â”‚       â”‚   â”œâ”€â”€ cleanup.py         # Data cleaning and preparation logic
â”‚       â”‚   â”œâ”€â”€ config_loader.py   # YAML configuration file parser
â”‚       â”‚   â”œâ”€â”€ databricks_utils.py# Databricks workspace interaction utilities
â”‚       â”‚   â””â”€â”€ uploader.py        # Uploads raw datasets to Databricks Volumes
â”‚       â”‚
â”‚       â”œâ”€â”€ feature/               # Feature engineering and transformation logic
â”‚       â”‚   â””â”€â”€ data_processor.py  # Pipeline for generating ML-ready features
â”‚       â”‚
â”‚       â”œâ”€â”€ model/                 # Model definition, training, and registration
â”‚       â”‚   â”œâ”€â”€ basic_model.py     # Baseline logistic regression model
â”‚       â”‚   â”œâ”€â”€ custom_model.py    # Custom model variant with hyperparameter tuning
â”‚       â”‚   â””â”€â”€ feature_lookup_model.py # Model integrating with Databricks Feature Store
â”‚       â”‚
â”‚       â”œâ”€â”€ serving/               # Model serving and API interface
â”‚       â”‚   â””â”€â”€ model_serving.py   # Databricks Serving endpoint handler
â”‚       â”‚
â”‚       â”œâ”€â”€ utils/                 # Generic utility functions and helpers
â”‚       â”‚   â”œâ”€â”€ config.py          # Configuration loader and management
â”‚       â”‚   â”œâ”€â”€ databricks_utils.py# Shared Databricks API wrappers
â”‚       â”‚   â”œâ”€â”€ env_loader.py      # Environment variable and .env manager
â”‚       â”‚   â””â”€â”€ timer.py           # Performance and timing utilities
â”‚       â”‚
â”‚       â””â”€â”€ visualization/         # Model monitoring and visualization tools
â”‚           â””â”€â”€ monitoring.py      # Metrics visualization and drift monitoring
â”‚
â”œâ”€â”€ tests/                         # Full testing suite (unit / integration / functional)
â”‚   â”œâ”€â”€ unit_test/                 # Unit tests for all modules
â”‚   â”œâ”€â”€ integration/               # Integration tests for Databricks & MLflow
â”‚   â””â”€â”€ functional/                # End-to-end tests (model serving and API)
â”‚
â”œâ”€â”€ uv.lock                        # Locked dependency versions (managed by UV)
â””â”€â”€ version.txt                    # Current project version number
```

## ğŸš€ Key Features

* **End-to-End ML Lifecycle:** Data upload â†’ Feature Engineering â†’ Training â†’ Registry â†’ Serving
* **Asset Bundle Deployment:** Automated workflows defined in `databricks.yml`
* **Environment-Aware Configuration:** Per-env catalog/schema setup (`dev`, `acc`, `prd`)
* **Testing:** Full unit, integration, and functional coverage using Pytest
* **Docs:** Built automatically via CI from `docs/`
* **Code Quality:** Pre-commit hooks, linting, and commit message enforcement via Commitizen
* **Business Integration**: We integrate a Power BI Dashboard that uses the results of the model prediction to demonstrate the business impact of this project in the decision making.

## âš™ï¸ Databricks Asset Bundle Workflow

The deployment and automation pipeline is defined in `databricks.yml`.
It orchestrates the following Databricks tasks:

1. **Preprocessing** â€” Runs `scripts/01.process_new_data.py`
2. **Model Training** â€” Runs `scripts/02.train_register_model.py`
3. **Conditional Deployment** â€” Deploys only if a new model version is created
4. **Serving Update** â€” Uses `scripts/03.deploy_model_serving.py`
5. **Post-Commit Check** â€” Optionally validates CI integration results

You can run the workflow directly from CLI:

```bash
databricks bundle deploy
databricks bundle run deployment --target dev
```

## ğŸ§ª Development & Testing Workflow

| Command                        | Description                                |
| ------------------------------ | ------------------------------------------ |
| `task dev-install`             | Setup dev dependencies                     |
| `task demo`                    | Run full demo pipeline locally             |
| `task run-upload-data`         | Upload dataset to Databricks volume        |
| `task run-process-data`        | Create train/test Delta tables             |
| `task train-register-model`    | Train and register baseline model          |
| `task fe_train_register_model` | Train with Feature Store & Feature Lookup  |
| `task lint`                    | Run Ruff, formatters, and pre-commit hooks |
| `task clean`                   | Clean environment and temporary files      |
| `pytest`                       | Run all unit/integration/functional tests  |


## ğŸ” Reproduce Results

1. Upload dataset:

   ```bash
   task run-upload-data
   ```
2. Train & register:

   ```bash
   task train-register-model
   ```
3. Deploy serving endpoint:

   ```bash
   task deploy-model-serving
   ```
4. Validate predictions:

   ```bash
   pytest tests/functional
   ```

## ğŸ§± Prerequisites

* **Required:** macOS/Linux, Python â‰¥3.12, Databricks workspace, `task`, `devbox`, `uv`
* **Optional:** Docker (for isolated testing), CI/CD setup with GitHub or GitLab runners

## ğŸ§¾ Configuration Example

```yaml
dev:
  catalog_name: mlops_dev
  schema_name: hotel_operation
  volume_name: data
  raw_data_file: "Hotel_Reservations.csv"
  train_table: hotel_reservations_train_set
  test_table: hotel_reservations_test_set
  feature_table_name: hotel_reservations_features
  feature_function_name: hotel_reservations_feature_fn
  experiment_name_fe: /Shared/hotel_reservations/fe_experiment
```

Switch environments easily:

```bash
task run-upload-data --branch=dev
task fe_train_register_model --branch=prd
```

## ğŸ“Š End-to-End Workflow

```mermaid
flowchart TD
    A[Raw Data CSV] --> B[Upload to Databricks Volume]
    B --> C[Process Data â†’ Train/Test Delta Tables]
    C --> D["Feature Engineering Pipeline (optional)"]
    D --> E["Feature Store Table + Function (optional)"]
    E --> F["Model Training (Logistic Regression)"]
    F --> G["MLflow Tracking (params, metrics, artifacts)"]
    G --> H[MLflow Registry / Unity Catalog]
    H --> I[Model Serving Deployment]
    I --> J[Batch & Online Prediction]
```

## ğŸ§‘â€ğŸ’» Contributing

```bash
git checkout -b feature/<your-feature>
task lint
cz commit
git push origin feature/<your-feature>
```

Then open a **Merge Request / Pull Request** via GitHub or GitLab.

Refer to the [CONTRIBUTING](CONTRIBUTING) file for full contribution guidelines.

## ğŸ“š Documentation

* ğŸ“˜ [View Wiki](https://github.com/end-to-end-mlops-databricks-4/marvelous-databricks-course-malganis35/blob/main/wiki-content/Course-Overview.md)
* ğŸ“— [Read the Docs](https://docs.mlops.caotri.dofavier.fr/)
* ğŸ§¾ Reports: `/docs/reports/figures/`

## ğŸ“œ License & Credits

Proprietary Â© 2025 â€” *Cao Tri Do*
For **internal use only**. See the [LICENCE](LICENCE) file for details.

This repository was initiated thanks to the *Marvelous MLOps â€” End-to-end MLOps with Databricks: https://maven.com/marvelousmlops/mlops-with-databricks*.

**I would like to deeply thanks my 2 Course Mentors**:

- [Maria Vechtomova (@mvechtomova)](https://github.com/mvechtomova)
- [Basak Tugce Eskili (basakeskili)](https://github.com/basakeskili)
